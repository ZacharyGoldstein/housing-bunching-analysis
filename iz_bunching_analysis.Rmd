---
title: "IZ Bunching Analysis"
author: "Zachary Goldstein"
date: "2023-03-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
library(readxl)
library(usdata)
library(magrittr)
library(rdd)
library(kableExtra)
library(poweRlaw)
library(fitdistrplus)
library(sads)
library(FatTailsR)
library(tidyverse)
options(dplyr.summarise.inform = FALSE)
```

# Loading Data

```{r}
df_cl = read_csv("data/corelogic_pb2_mf.csv")
dim(df_cl)
head(df_cl)
```
```{r}
df_iz_raw = read_excel("ih.org-inclusionary-database/Local Inclusionary Housing Policies.xlsx",
                 sheet="data",skip=1)
head(df_iz_raw)
```
# Cleaning/Processing

```{r}
df_pops = read_csv("https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv") %>% 
  mutate(state_code=state2abbr(State))
head(df_pops)
```

```{r}
df_iz = df_iz_raw %>% 
  select(`Program ID`,`Program Name`,`Government Name`,`Government Type`,
        state=`Government Address: State`,
        `Year Program Adopted`,`Year Program Updated`,
         `Program Applicable Areas`,`Minimum Project Size`,`Minimum Set-aside`,
         `Incentive`) %>% 
  rename_with(\(x){str_replace_all(x," ","_") %>% str_to_lower()}) %>% 
  # Only keep policies with known thresholds (3 units or less doesn't count)
  mutate(minimum_project_size = ifelse(minimum_project_size %in% c("Don't know","N/A","1 unit","2 units","3 units"),"missing",str_to_lower(minimum_project_size)),
         government_name = str_replace(government_name,
    "Town of |City of |City and County of |City Of |Town Of | Borough|Borough of ","") %>% 
      str_replace("Newark City","Newark"),
         state=str_replace(state,"D.C.","DC"),
    # Manual override policy details based on research
    # https://www.fbm.com/publications/what-you-need-to-know-about-san-franciscos-inclusionary-affordable-housing-program/
    minimum_project_size=ifelse(government_name=='San Francisco','25 units',minimum_project_size),
    year_program_adopted=ifelse(government_name=='San Francisco','2016',year_program_adopted)
    ) %>% 
  filter(minimum_project_size!="missing") %>% 
  left_join(df_pops %>% select(City,state_code,city_pop=Population),
            by=c("government_name"="City","state"="state_code"))

dim(df_iz)
head(df_iz)
```
```{r}
df_iz %<>% 
  mutate(minimum_project_size=str_replace(minimum_project_size,"1 million","1000000") %>% 
           str_replace(",",""),
  threshold_cat=case_when(
    str_detect(minimum_project_size,"^\\d+ units$") ~ "units",
    str_detect(minimum_project_size,"^\\d+ acres \\(parcel\\)$") ~ "acres_parcel",
    str_detect(minimum_project_size,"^\\d+ sqft \\(parcel\\)$") ~ "sqft_parcel",
    str_detect(minimum_project_size,"^\\d+ sqft \\(building\\)$") ~ "sqft_building",
    T ~ "other")) %>% 
  filter(threshold_cat!='other') %>% 
  mutate(threshold_num = str_extract(minimum_project_size, "[0-9]+\\.*[0-9]*") %>% 
           as.numeric)
df_iz %>% 
  distinct(minimum_project_size,threshold_cat,threshold_num) %>% 
  arrange(threshold_cat,minimum_project_size)
```
```{r}
# Remove duplicate municipalities in IZ dataset
df_iz %>% 
  count(government_name,state) %>%
  arrange(desc(n)) %>% 
  head()

# Need to come up with better way to pick among duplicates
df_iz %<>% 
  arrange(desc(threshold_num)) %>% 
  distinct(government_name,state,.keep_all = T)
```

```{r}
dim(df_iz)
head(df_iz)
```

## Single City Datasets
Combine CoreLogic with other property datasets


### Portland, OR
```{r}
# Portland OR isn't in the non-historical CL at all so can easily combine without de-duping
df_pd = read_csv("data/PDX_Residential_Building_Permits_20230328.csv",
      col_select =c("STATUS","YEAR_","NEWCLASS","NEWTYPE","NEW_UNITS",
                           "IS_ADU")) %>% 
      filter(NEW_UNITS>=2,IS_ADU==F) %>% 
      mutate(municipality_name="PORTLAND",
             situs_city="PORTLAND",
             situs_county="MULTNOMAH",
             situs_state="OR",
             number_of_buildings=1,
             number_of_units=NEW_UNITS,
             fips_code="41051",
             year_built=YEAR_) %>% 
      select(any_of(names(df_cl)))
```
### San Diego
```{r}
max_year_cl_sd = df_cl %>% 
                filter(situs_city=='SAN DIEGO',situs_state=='CA') %>% 
                pull(year_built) %>% 
                max(na.rm=T)
df_sd_raw = read_csv("data/san_diego_parcels.csv",
                     col_select = c("UNITQTY","YEAR_EFFECTIVE"),
                     show_col_types=F)
df_sd = df_sd_raw %>% 
   mutate(municipality_name="SAN DIEGO",
             situs_city="SAN DIEGO",
             situs_county="SAN DIEGO",
             situs_state="CA",
            # situs_zip_code=SITUS_ZIP,
             number_of_units=UNITQTY,
             number_of_buildings=1,
             fips_code="06073",
             year_built=ifelse(as.numeric(YEAR_EFFECTIVE)<=23,
                               2000+as.numeric(YEAR_EFFECTIVE),
                               1900+as.numeric(YEAR_EFFECTIVE))) %>% 
      select(any_of(names(df_cl))) %>% 
      filter(number_of_units>=2,year_built>max_year_cl_sd)
head(df_sd)
```
### San Francisco
```{r}
df_sf_raw = read_csv("data/sf_building_permits_20230404.csv",
                     show_col_types=F,
    col_select = c('Permit Type Definition','Description','Existing Use','Proposed Use',
                              'Permit Creation Date',
                             'Current Status',
                              'Current Status Date',
  'Filed Date','Issued Date','Completed Date','First Construction Document Date',
                             'Existing Units','Proposed Units'))
df_sf = df_sf_raw %>% 
   mutate(municipality_name="SAN FRANCISCO",
             situs_city="SAN FRANCISCO",
             situs_county="SAN FRANCISCO",
             situs_state="CA",
             number_of_units=`Proposed Units`,
             number_of_buildings=1,
             fips_code="06075",
          # Using filed date instead of year built because it makes more sense 
          # based on expected IZ developer behavior
            date_built = as.Date(`Filed Date`,format="%m/%d/%Y"),
             year_built=as.numeric(format(as.Date(`Filed Date`, format = "%m/%d/%Y"), format = "%Y"))) %>% 
      filter(number_of_units>=2,str_detect(`Permit Type Definition`,"new construction")) %>% 
      select(any_of(names(df_cl)),date_built)
head(df_sf)
```
### San Jose
```{r}
# Just active building permits, so shouldn't overlap with CL
df_sj_raw = read_csv("data/san_jose_buildingpermitsactive.csv",
                     show_col_types=F)
df_sj = df_sj_raw %>% 
   mutate(municipality_name="SAN JOSE",
             situs_city="SAN JOSE",
             situs_county="SANTA CLARA",
             situs_state="CA",
             number_of_units=DWELLINGUNITS,
             number_of_buildings=1,
             fips_code="06085",
             date_built = as.Date(ISSUEDATE,format="%m/%d/%Y"),
             year_built=as.numeric(format(as.Date(ISSUEDATE, format = "%m/%d/%Y"), format = "%Y"))) %>% 
      filter(number_of_units>=2,WORKDESCRIPTION=='New Construction') %>% 
      select(any_of(names(df_cl)),date_built)
head(df_sj)
```

### Denver

```{r}
df_dn_raw = read_csv("data/denver_real_property_residential_characteristics.csv",
                     show_col_types=F,col_select=c("UNITS","CCYRBLT"))
df_dn = df_dn_raw %>% 
   mutate(municipality_name="DENVER",
             situs_city="DENVER",
             situs_county="DENVER",
             situs_state="CO",
             number_of_units=UNITS,
             number_of_buildings=1,
             fips_code="08031",
             year_built=CCYRBLT) %>% 
      filter(number_of_units>=2) %>% 
      select(any_of(names(df_cl)))
head(df_dn)
```

### Minneapolis

```{r}
max_year_cl_mn = df_cl %>% 
                filter(situs_city=='MINNEAPOLIS',situs_state=='MN') %>% 
                pull(year_built) %>% 
                max(na.rm=T)
df_mn_raw = read_csv("data/minneapolis_permits_20230412.csv",
                     show_col_types=F,col_select=c("dwellingUnitsNew","issueDate"))
df_mn = df_mn_raw %>% 
   mutate(municipality_name="MINNEAPOLIS",
             situs_city="MINNEAPOLIS",
             situs_county="HENNEPIN",
             situs_state="MN",
             number_of_units=dwellingUnitsNew,
             number_of_buildings=1,
             fips_code="27053",
             date_built = as_date(ymd_hms(issueDate)),
             year_built= year(ymd_hms(issueDate))) %>% 
      filter(number_of_units>=2,year_built>max_year_cl_mn) %>% 
      select(any_of(names(df_cl)),date_built)
head(df_mn)
```

### New Jersey
```{r}
# There's no NJ at all in CL, so no duplicates
df_nj_raw = read_csv("data/NJ_permits_20230412.csv",
                     show_col_types=F,col_select=c("Sale Units Gained",
          "Rental Units Gained","Municipality Name","County","Permit Date"))
df_nj = df_nj_raw %>% 
   mutate(municipality_name=`Municipality Name`,
             situs_city=`Municipality Name`,
             situs_county=`County`,
             situs_state="NJ",
             number_of_units=`Sale Units Gained`+`Rental Units Gained`,
             number_of_buildings=1,
            # fips_code=,
             date_built = mdy(`Permit Date`),
             year_built= year(mdy(`Permit Date`))) %>% 
      filter(number_of_units>=2) %>% 
      select(any_of(names(df_cl)),date_built)
head(df_nj)
```
### Honolulu
```{r}
df_ho_raw = read_csv("data/honolulu_permits_20221231.csv",
                     show_col_types=F,col_select=c(
      #"issuedate","statusdescription","proposeduse","numunitsdel",
      "createddate","numunitsadd","newbuilding","structurecode"))
df_ho = df_ho_raw %>% 
   mutate(municipality_name="OAHU",
             situs_city="HONOLULU",
             situs_county="OAHU",
             situs_state="HI",
             number_of_units=`numunitsadd`,
             number_of_buildings=1,
            fips_code="15003",
            date_built = mdy(`createddate`),
             year_built= year(mdy(`createddate`))) %>% 
      filter(newbuilding=='Y',structurecode=="53 - APARTMENT (3 OR MORE)",
             numunitsadd>=3) %>% 
   select(any_of(names(df_cl)),date_built)
head(df_ho)
```

### Fairfax, VA
```{r}
df_ff_raw = read_csv("data/fairfax_housing_units.csv")
df_ff = df_ff_raw %>% 
  # Exclude single family homes and mobile homes
  filter(HOUSI_UNIT_TYPE %in% c("SF","MH")==F) %>% 
  # Group units at same location into buildings
  group_by(X,Y) %>% 
  summarize(number_of_units=sum(CURRE_UNIT),
            year_built=median(YEAR_BUILT)) %>% 
  mutate(situs_county="FAIRFAX",
             situs_state="VA",
             number_of_buildings=1,
            fips_code="51059") %>% 
      filter(number_of_units>=2) %>% 
  ungroup %>% 
  select(any_of(names(df_cl)))
head(df_ff)
```
### Montgomery, MD
```{r}
# Not in CL at all so don't need to worry about dupes
df_mg_raw = read_csv("data/montgomery_housing_reg.csv")
df_mg = df_mg_raw %>% 
  select(`Property Street Address`,
         `Property City`,
         `Structure Type`,
         `Unit Count`,`Year Built`) %>% 
  filter(str_detect(`Structure Type`,'Accessory')==F,
         `Structure Type`!='Single Family') %>% 
  group_by(`Property Street Address`,`Property City`,year_built=`Year Built`) %>% 
  summarize(number_of_units=sum(`Unit Count`)) %>% 
  filter(number_of_units>=2) %>% 
  mutate(municipality_name=`Property City`,
         situs_city=`Property City`,
         situs_county="MONTGOMERY",
             situs_state="MD",
             number_of_buildings=1,
            fips_code="24031") %>% 
  ungroup %>% 
  select(any_of(names(df_cl)))
df_mg %>% 
  head()
```

## Data Quality Check

Compare CoreLogic to other data sources
```{r}
# SF
df_sf %>% 
  head()
df_cl_sf_recent = df_cl %>% 
  filter(situs_city=='SAN FRANCISCO',
         year_built>= (df_sf %>% pull(year_built) %>% min))
df_cl_sf_recent %>% 
  head()
```
```{r}
df_cl_sf_recent %>% nrow
```

```{r}
df_sf %>% nrow
```

```{r}
df_cl_sf_recent %>% 
  filter(number_of_units<=50,number_of_units>=4) %>% 
  ggplot(aes(x=number_of_units)) +
  geom_histogram(binwidth=1,color='white')
```

## Data Merging
```{r}
# Exclude cities where I prefer to just use non-corelogic dataset to avoid dupes
cities_to_exclude_from_cl = "SAN FRANCISCO"
min_year_ho = df_ho$year_built %>% min
df_properties = df_cl %>% 
  filter(situs_city %in% cities_to_exclude_from_cl == F,
         situs_city!="HONOLULU"|year_built<min_year_ho,
         situs_county!='FAIRFAX'|situs_state!='VA') %>% 
  bind_rows(df_pd) %>% 
  bind_rows(df_sd) %>% 
  bind_rows(df_sf) %>% 
  bind_rows(df_sj) %>% 
  bind_rows(df_dn) %>% 
  bind_rows(df_mn) %>% 
  bind_rows(df_nj) %>% 
  bind_rows(df_ho) %>% 
  bind_rows(df_ff) %>% 
  bind_rows(df_mg)
df_properties %>% 
  tail()
```


```{r}
# Check for duplicates and missing values (linkage failures)
df_merged = df_properties %>% 
  mutate(situs_city=coalesce(situs_city,municipality_name),
         situs_county=str_replace(situs_county," COUNTY","")) %>% 
  full_join(df_iz %>% 
              filter(government_type %in% c("Town or city","Special district")) %>% 
              mutate(government_name=str_to_upper(government_name)),
        by=c("situs_city"="government_name","situs_state"="state"),keep=T) %>% 
  full_join(df_iz %>% 
              filter(government_type %in% c("Consolidated city-county","County")) %>% 
              mutate(government_name=str_to_upper(government_name) %>% 
                       str_replace("COUNTY OF ","") %>% 
                       str_replace(" COUNTY","")),
            by=c("situs_county"="government_name","situs_state"="state"),keep=T,suffix=c("_xcity","_ycounty"))
# Assert there are no properties that match to more than 1 IZ policy
# Need to address edge cases here, like San Mateo County
# stopifnot(df_merged %>% filter(is.na(government_name_xcity)==F&is.na(government_name_ycounty)==F) %>% nrow == 0)
# df_merged %>% filter(is.na(government_name_xcity)==F&is.na(government_name_ycounty)==F) %>% 
#   distinct(government_name_xcity,government_name_ycounty)

# Coalesce cols together
cols_to_coalesce = names(df_merged)
for (i in seq_along(cols_to_coalesce)){
  x_col_name = cols_to_coalesce[i]
  if (str_ends(x_col_name,"_xcity")){
    new_col_name = str_replace(x_col_name,"_xcity","")
    y_col_name = str_replace(x_col_name,"_xcity","_ycounty")
    df_merged = df_merged %>% 
      mutate(!!new_col_name := coalesce(!!sym(x_col_name), !!sym(y_col_name)))
  }}

df_merged = df_merged %>% select(!contains("_xcity")&!contains("_ycounty"))
df_merged %>% 
  select(situs_city,situs_county,situs_state,government_name,government_type,state) %>% 
  filter(is.na(government_name)==F) %>% 
  distinct(situs_city,situs_county,situs_state,government_name,government_type,state) %>% 
  slice_sample(n=20)
```

## Missing data check
Check for IZ policies with missing property data

Note: I looked for the top 10 missing IZ cities by city pop and didn't find in non-historical CL data
```{r}
df_missing = df_merged %>% 
  filter(is.na(clip)) %>% 
  distinct(government_name,government_type,state,city_pop) %>% 
  arrange(desc(city_pop))
df_missing %>% 
  head(10)
```

## Edge cases to handle
Handle San Mateo, Santa Clara edge cases with double requirement at city + county level
```{r}
df_merged %>% filter(str_detect(situs_county,"SAN MAT")) %>% 
  select(situs_city,situs_county,situs_state,government_name) %>% 
  distinct %>% 
  head()
```

## Prep data for analysis
```{r}
df_analysis = df_merged %>% 
  mutate(bldg_year_combined=coalesce(effective_year_built,year_built),
         bldg_sq_ft_combined = coalesce(universal_building_square_feet,building_square_feet)) %>% 
  filter(is.na(bldg_year_combined)==F,is.na(threshold_cat)==F,number_of_buildings==1,
         # Either date of adoption unknown, or the building was built after IZ adoption
         (bldg_year_combined>=year_program_adopted|year_program_adopted=="Don't know")) %>% 
  select(situs_city,situs_county,situs_state,government_name,government_type,
        acres,land_square_footage,bldg_sq_ft_combined,
        bldg_year_combined,number_of_units,number_of_buildings,
        threshold_cat,threshold_num,year_program_adopted,year_program_updated)
df_analysis %>% 
  slice_sample(n=10)
df_analysis %>% dim
```

```{r}
df_analysis %>% 
  count(situs_city,situs_county,situs_state,threshold_cat,threshold_num) %>% 
  arrange(desc(n)) %>% 
  head(10)
```


```{r}
df_units = df_analysis %>% 
  filter(threshold_cat=="units",threshold_num>=5) %>% 
  mutate(rel_units=number_of_units-threshold_num,
         units_below_thresh=threshold_num-number_of_units) %>% 
  # Limit to counties where we have at least n distinct unit counts
  inner_join(df_analysis %>% 
  group_by(situs_county,situs_state) %>% 
  summarize(nunique_unit_counts = length(unique(number_of_units))) %>% 
  filter(nunique_unit_counts>=6) %>% 
  select(situs_county,situs_state),
    by=c("situs_county","situs_state")) 

dim(df_units)
df_units %>% head()
```



# EDA

## Counts

```{r}
# How many IZ programs are in the inclusionary housing spreadsheet?
df_iz_raw %>% nrow
```

```{r}
# How many IZ programs in the spreadsheet have a threshold?
df_iz %>% 
  count(threshold_cat) %>% 
  arrange(desc(n))
```

```{r}
# How many unique jurisdictions have IZ programs with a threshold?
df_iz %>% 
  pull(government_name) %>% 
  unique() %>% 
  length
```

```{r}
# How many IZ jurisdictions overlap with Property Data (CL + other)?
df_merged %>% 
  filter(is.na(number_of_units)==F) %>% 
  pull(government_name) %>% 
  unique() %>% 
  length
```

```{r}
# How many IZ jurisdictions overlap with Property Data (CL + other) if we limit to 
# properties from after the start of the IZ program?
df_analysis %>% 
  pull(government_name) %>% 
  unique() %>% 
  length
```

```{r}
# How many properties are in that dataset?
df_analysis %>% nrow
```

```{r}
# And which cities are they in?
df_analysis %>% 
  count(situs_city,situs_state) %>% 
  arrange(desc(n)) %>% 
  head(10)
```
## Units Table
```{r}
df_units %>% select(government_name,situs_state) %>% n_distinct
```

```{r}
df_units %>% nrow
```

```{r}
df_units %>% count(situs_state) %>% arrange(desc(n)) %>% mutate(pct=round(100*n/nrow(df_units))) %>% head()
```

```{r}
df_units %>% distinct(government_name,situs_state) %>% count(situs_state) %>% arrange(desc(n)) %>% 
  mutate(pct=round(100*n/n_distinct(df_units %>% 
       select(government_name,situs_state)))) %>% head()
```

```{r}
df_units %>% count(situs_city,situs_state) %>% arrange(desc(n)) %>% 
  mutate(pct=round(100*n/nrow(df_units),1)) %>% 
  head()
```


# National Analysis

## Unit Threhsolds

```{r}
df_units %>% count(threshold_num) %>% 
  ggplot(aes(x=threshold_num,y=n)) +
  geom_bar(stat='identity') +
  labs(x = 'Unit Threshold',
       y = 'Buildings in Dataset',
       title = 'Distribution of Unit Thresholds by Building')
```

```{r}
df_units %>% 
  select(government_name,situs_state,threshold_num) %>% 
  distinct %>% 
  count(threshold_num) %>% 
  ggplot(aes(x=threshold_num,y=n)) +
  geom_bar(stat='identity') +
  labs(x = 'Unit Threshold',
       y = 'Programs in Dataset',
       title = 'Distribution of Unit Thresholds by Program')
```


```{r}
ggplot(df_units %>% filter(number_of_units<=50,number_of_units>=3),
       aes(x=number_of_units)) +
  geom_histogram(binwidth=1,color='white') +
  labs(x = 'Number of Units',y='Frequency',
       title = 'Distribution of Building Unit Counts')
```

```{r}
ggplot(df_units %>% filter(rel_units<=50,rel_units>=-50),
       aes(x=rel_units)) +
  geom_histogram(binwidth=1,color='white') + 
  labs(x = 'Units Relative to Threshold',
       y = 'Frequency',
       title = 'Relative Unit Count Frequencies')
```

### Inference

#### McCrary Test

```{r}
DCdensity(runvar=df_units %>% pull(rel_units),cutpoint=0,ext.out=T)
```

### Group by Threshold

```{r}
df_units = df_units %>% 
  mutate(threshold_group = case_when(threshold_num<=9~"A 5-9",
                                     threshold_num<=14~"B 10-14",
                                     threshold_num<=19~"C 15-19",
                                     threshold_num<=24~"D 20-24",
                                     threshold_num<=29~"E 25-29",
                                     threshold_num==30~"F 30",
                                     threshold_num==50~"G 50",
                                     threshold_num==100~"H 100"))
df_units %>% count(threshold_group)
```

#### Plots

```{r}
df_units %>% 
  filter(rel_units<=50,rel_units>=-50) %>% 
  ggplot(aes(x=rel_units)) +
  geom_histogram(binwidth=1) +
  facet_wrap(~threshold_group,scales='free')
```

#### Inference

```{r}
for (group in df_units$threshold_group %>% unique %>% sort){
  df_group = df_units %>% filter(threshold_group==group)
  if (group %in% c("A 5-9","C 15-19","D 20-24","H 100")){next}
  p = DCdensity(runvar=df_group %>% pull(rel_units),cutpoint=0,ext.out=F) %>% 
    round(5)
  #p = results$p
  print(c(group,p))
}
```


### Group by City

```{r}
df_unit_bunch = df_units %>% 
  mutate(bunch_ind = as.numeric((units_below_thresh==1)|
           (units_below_thresh==2)&(threshold_num %% 2 == 0)),
         pre_bunch_ind = as.numeric(
           ((units_below_thresh %in% c(3,4))&(threshold_num %% 2 == 0))|
           ((units_below_thresh %in% c(2,3))&(threshold_num %% 2 == 1))
           ),
         post_bunch_ind = as.numeric(units_below_thresh==0),
         one_below_ind = as.numeric(units_below_thresh==1),
         two_below_ind = as.numeric(units_below_thresh==2)
         ) %>% 
  filter(threshold_num>=6)
bunching_cities_table = df_unit_bunch %>% 
  filter(government_type %in% c("Town or city","Special district")) %>% 
  group_by(situs_city,situs_state,government_type,threshold_num) %>% 
  summarize(n_bunch=sum(bunch_ind),
            n_pre_bunch = sum(pre_bunch_ind),
            n_post_bunch = sum(post_bunch_ind),
            n_one_below = sum(one_below_ind),
            n_two_below = sum(two_below_ind),
            n_total=n(),
            n_diff=n_bunch-n_pre_bunch) %>% 
  arrange(desc(n_diff)) %>% 
  # If IZ takes effect at unit counts >=X...
  # ... there should be more buildings of size X-1 or X-2 than of X in the jurisdiction
  filter(n_one_below>n_post_bunch|n_two_below>n_post_bunch) %>% 
  select(-n_one_below,-n_two_below) %>% 
  head(10)

```

```{r}
bunching_counties_table = df_unit_bunch %>% 
  filter(government_type %in% c("County","Consolidated city-county")) %>% 
  group_by(situs_county,situs_state,government_type,threshold_num) %>% 
  summarize(n_bunch=sum(bunch_ind),
            n_pre_bunch = sum(pre_bunch_ind),
            n_post_bunch = sum(post_bunch_ind),
            n_one_below = sum(one_below_ind),
            n_two_below = sum(two_below_ind),
            n_total=n(),
            n_diff=n_bunch-n_pre_bunch) %>% 
  arrange(desc(n_diff)) %>% 
  # If IZ takes effect at unit counts >=X...
  # ... there should be more buildings of size X-1 than of X in the jurisdiction
  filter(n_one_below>n_post_bunch|n_two_below>n_post_bunch) %>% 
  select(-n_one_below,-n_two_below) %>% 
  head(10)
```

```{r}
bunching_cities_table %>% 
  rbind(bunching_counties_table) %>% 
  ungroup %>% 
  arrange(desc(n_diff)) %>% 
  mutate(`City/County`=coalesce(situs_city,situs_county)) %>% 
  relocate(`City/County`,.before = situs_state) %>% 
  rename(State=situs_state,`Unit Threshold`=threshold_num,
         `Government Type` = government_type,
         `N Bunch`=n_bunch,`N Pre-Bunch`=n_pre_bunch,`N Post-Bunch`=n_post_bunch,
         `Total Buildings`=n_total,`Bunch Minus Pre-Bunch`=n_diff) %>% 
  select(-c("situs_city","situs_county")) %>% 
  head(10) %>% 
  kable %>% 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                font_size = 12,
                position = "left")
```


```{r}
single_eda_plot = function(jurisdiction,state,data=df_units){
  data_filtered = df_units %>% 
    filter(situs_city==jurisdiction|situs_county==jurisdiction,
                                      situs_state==state,
                      rel_units<=50)
  thresh = data_filtered$threshold_num %>% first
ggplot(data_filtered,
       aes(x=number_of_units)) +
  geom_histogram(binwidth=1,color='white') +
  geom_vline(xintercept=thresh,color='red') + 
  labs(title=str_glue("Distribution of Apartment Building Unit Counts in {jurisdiction}, {state}"),
       y="Frequency",x = "Units per Apartment Building")
}
single_eda_plot(jurisdiction="FAIRFAX",state="VA")
```




## Area Thresholds
```{r}
sq_ft_per_acre = 43560
df_parcel_area = df_analysis %>% 
  filter(threshold_cat %in% c("acres_parcel","sqft_parcel")) %>% 
  mutate(threshold_acres = ifelse(threshold_cat=='acres_parcel',threshold_num,
                                  threshold_num/sq_ft_per_acre),
         rel_acres = acres-threshold_acres)
dim(df_parcel_area)
df_parcel_area %>% 
  head()
```

```{r}
df_parcel_area %>% 
  filter(acres<50) %>% 
  ggplot(aes(x=rel_acres)) +
  geom_histogram(color='white')
```

```{r}
df_building_area = df_analysis %>% 
  filter(threshold_cat=='sqft_building')
df_building_area %>% 
  head()
```

# Case Study ID
Promising candidates for single-city datasets

Cities missing from CL completely
```{r}
df_merged %>% 
  filter(is.na(clip),threshold_cat=="units",threshold_num>=6,city_pop>=100000,
         year_program_adopted<=2020) %>% 
  distinct(government_name,state,city_pop,threshold_num,threshold_cat) %>% 
  arrange(desc(city_pop))
```
Cities missing recent data from CL (post-IZ adoption)
```{r}
cl_has_post_adoption = df_merged %>% 
  filter((year_built>year_program_adopted|year_program_adopted=="Don't know"),city_pop>100000,threshold_num>=6,
         threshold_cat=="units",(year_program_adopted<=2020|year_program_adopted=="Don't know")) %>% 
  distinct(situs_city,situs_state,city_pop) %>% 
  arrange(desc(city_pop)) %>% 
  head(20)
```

```{r}
cl_has_any_time = df_merged %>% 
  filter(is.na(clip)==F,city_pop>100000,threshold_num>=6,
         threshold_cat=="units",year_program_adopted<=2020|year_program_adopted=="Don't know") %>% 
  distinct(situs_city,situs_state,city_pop) %>% 
  arrange(desc(city_pop)) %>% 
  head(20)
```

```{r}
cl_has_any_time %>% 
  left_join(cl_has_post_adoption,by=c("situs_city","situs_state")) %>% 
  filter(is.na(city_pop.y)==T)
```

```{r}
df_merged %>% 
  filter(government_type!='Town or city',threshold_cat=='units',threshold_num>=6,
         is.na(number_of_units),year_program_adopted<=2020|year_program_adopted=="Don't know") %>% 
  pull(program_id)
```

```{r}
df_iz %>% 
  filter(threshold_num>=10,threshold_cat=='units') %>% 
  arrange(desc(city_pop)) %>% 
  head()
```

```{r}
df_iz %>% filter(government_name=='San Francisco')
```

# Case Study Analysis
## San Francisco, CA

```{r}
df_sf %>% 
  filter(number_of_units>=2,
         number_of_units<=50,date_built>="2016-06-01") %>% 
  ggplot(aes(x=number_of_units)) + 
  geom_histogram(color='white',binwidth=1)
```
```{r}
min_units = 3
max_units = 100
sf_post_start_date = "2016-06-08"
sf_unit_counts = df_sf %>% 
  filter(number_of_units>=min_units,
         number_of_units<=max_units,date_built>=sf_post_start_date) %>% pull(number_of_units)
df_sf_hist = df_sf %>% 
  filter(number_of_units>=min_units,
         number_of_units<=max_units,date_built>=sf_post_start_date) %>% 
  count(number_of_units,name='freq') %>% 
  # Add missing zero frequencies
  complete(number_of_units=min_units:max_units,fill=list(freq=0)) %>% 
  mutate(number_of_units_inverse=1/number_of_units,
         number_of_units_inverse_squared = 1/number_of_units^2,
         freq_log = log(freq),
         number_of_units_log=log(number_of_units),
         freq_sqrt = sqrt(freq),
         number_of_units_sqrt=sqrt(number_of_units))
sf_counterfactual_max = 23
df_sf_hist_train = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max)

sf_counterfactual_model_inv = lm(freq~number_of_units_inverse+number_of_units_inverse_squared,
                             data=df_sf_hist_train)
sf_counterfactual_model_exp = nls(freq~a*exp(-b*number_of_units),
                            data=df_sf_hist_train, start = list(a = 1000, b = 0.5))
sf_counterfactual_model_pow = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train, start = list(a = 1000, b = 0.5))

df_sf_hist$counterfactual_freq_inv = predict(sf_counterfactual_model_inv,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_exp = predict(sf_counterfactual_model_exp,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow = predict(sf_counterfactual_model_pow,df_sf_hist) %>% round(1)

df_sf_hist
```
```{r}
df_sf_hist %>% 
  filter(number_of_units>=3,number_of_units<=100) %>% 
  ggplot() +
  geom_bar(aes(x=number_of_units,y=freq),stat='identity') +
  geom_line(aes(x=number_of_units,y=counterfactual_freq_pow),color='blue') +
  geom_vline(xintercept=23.5,linetype='dashed',color='blue') +
  geom_vline(xintercept=24.5,linetype='dashed',color='black') +
  scale_x_continuous(n.breaks=10) +
  labs(x = 'Number of Units per Building',
       y = 'Frequency',
       title = 'Frequency of Apartment Building Unit Counts in SF, 6/2016-4/2023')
```



### Interpolation Approach

```{r}
df_sf_hist_train_interpol_0 = df_sf_hist
df_sf_hist_train_interpol_2 = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max|number_of_units>=sf_counterfactual_max*1.5)
df_sf_hist_train_interpol_3 = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max|number_of_units>=sf_counterfactual_max*2)
df_sf_hist_train_interpol_4 = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max|number_of_units>=sf_counterfactual_max*3)

sf_counterfactual_model_pow_interpol_0 = nls(freq~ a*number_of_units^(-b),
              data=df_sf_hist_train_interpol_0, start = list(a = 1000, b = .5))
sf_counterfactual_model_pow_interpol_2 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_2, start = list(a = 1000, b = 0.5))
sf_counterfactual_model_pow_interpol_3 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_3, start = list(a = 1000, b = 0.5))
sf_counterfactual_model_pow_interpol_4 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_4, start = list(a = 1000, b = 0.5))



df_sf_hist$counterfactual_freq_pow_interpol_0 = predict(sf_counterfactual_model_pow_interpol_0,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow_interpol_2 = predict(sf_counterfactual_model_pow_interpol_2,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow_interpol_3 = predict(sf_counterfactual_model_pow_interpol_3,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow_interpol_4 = predict(sf_counterfactual_model_pow_interpol_4,df_sf_hist) %>% round(1)

df_sf_hist$counterfactual_freq_pow_interpol_5 = map_vec(df_sf_hist$number_of_units,
                                                        \(x){300*x^(-1.2)})

df_sf_hist
```

```{r}
df_sf_hist %>% 
  filter(number_of_units>=3,number_of_units<=100) %>% 
  ggplot() +
  geom_bar(aes(x=number_of_units,y=freq),stat='identity') +
  geom_line(aes(x=number_of_units,y=counterfactual_freq_pow_interpol_0),color='blue') +
  geom_line(aes(x=number_of_units,y=counterfactual_freq_pow_interpol_5),color='red') +
  geom_vline(xintercept=23.5,linetype='dashed',color='blue') +
  geom_vline(xintercept=24.5,linetype='dashed',color='black') +
  scale_x_continuous(n.breaks=10) +
  labs(x = 'Number of Units per Building',
       y = 'Frequency',
       title = 'Frequency of Apartment Building Unit Counts in SF, 6/2016-4/2023')
```

Interpolation only slightly increases fatness of tails. Need to try different 
distribution.
```{r}
df_sf_hist %>% 
  summarize(n_actual=sum(number_of_units*freq),
            n_counterfactual=sum(number_of_units*counterfactual_freq_pow),
            sum(number_of_units*counterfactual_freq_pow_interpol_2),
            sum(number_of_units*counterfactual_freq_pow_interpol_3),
            sum(number_of_units*counterfactual_freq_pow_interpol_4))
```

```{r}
sf_counterfactual_model_pow_interpol_0
```

```{r}
a = 1000
b = 1.2
x = 3:100
y = map_vec(x,.f=\(x){a*x^(-b)})
data.frame(x=x,y=y) %>% 
ggplot(aes(x=x,y=y)) + 
  geom_line()
```


### Fat Tails

```{r}

```

```{r}
# x = df_sf_hist_train %>% filter(freq!=0) %>% pull(freq)
# fit_x = fitdist(x,"pareto",start=list(scale=1,shape=1))
# k = fit$estimate[1]
# a = fit$estimate[2]
# log_x = log(x ~ k)
# log_y = log(a) - a * log_x
# fit_reg = lm(log_y~log_x)
# plot(x, a*k^a / x^(a+1), type = "l", lwd = 2, col = "blue")
# lines(x, dpareto(x, scale = k, shape = a), type = "l", lwd = 2, col = "red")
# plot(log_x + k, log_y, pch = 16)
# abline(fit_reg, col = "red")
```


```{r}
# sf_counterfactual_model_ft = dislnorm$new(dat=df_sf_hist_train %>%
#            filter(freq!= 0) %>% pull(freq))
# NlS
# cf = counterfactual, ft = fat tails
sf_cf_mod_ft_1 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_0, start = list(a = 1000, b = 0.5))
df_sf_hist$counterfactual_freq_ft_1 = predict(sf_cf_mod_ft_1,df_sf_hist) %>% round(1)
# FatTailsR
sf_cf_mod_ft_2_reg = regkienerLX(X=sf_unit_counts)
#sf_cf_mod_ft_2 = sf_cf_mod_ft_2_reg$regk0

# use regk0 to get nls object?
#df_sf_hist$counterfactual_freq_ft_2 = predict(sf_cf_mod_ft_2) %>% round(1)

# other options:

```
#### FatTailsR

```{r}
sf_cf_mod_ft_2_reg = regkienerLX(X=sf_unit_counts)
```


```{r}
plot(sf_cf_mod_ft_2_reg$dfrXP)
```

```{r}
plot(sf_cf_mod_ft_2_reg$dfrEP)
```

```{r}
plot(sf_cf_mod_ft_2_reg$dfrED)

```

```{r}
sf_cf_mod_ft_k1 = regkienerLX(X=sf_unit_counts,model="K1")
plot(sf_cf_mod_ft_k1$dfrXP)
plot(sf_cf_mod_ft_k1$dfrEP)
plot(sf_cf_mod_ft_k1$dfrED)
```

```{r}
sf_cf_mod_ft_k2 = regkienerLX(X=sf_unit_counts,model="K2")
plot(sf_cf_mod_ft_k2$dfrXP)
plot(sf_cf_mod_ft_k2$dfrEP)
plot(sf_cf_mod_ft_k2$dfrED)
```

```{r}
sf_cf_mod_ft_k3 = regkienerLX(X=sf_unit_counts,model="K3")
plot(sf_cf_mod_ft_k3$dfrXP)
plot(sf_cf_mod_ft_k3$dfrEP)
plot(sf_cf_mod_ft_k3$dfrED)
```

```{r}
sf_cf_mod_ft_k4 = regkienerLX(X=sf_unit_counts,model="K4")
plot(sf_cf_mod_ft_k4$dfrXP)
plot(sf_cf_mod_ft_k4$dfrEP)
plot(sf_cf_mod_ft_k4$dfrED)
```


```{r}
est_probs = reg$dfrEP %>%
  rowwise %>% 
  mutate(quantile_rounded=max(round(E),min(sf_unit_counts))) %>% 
  group_by(quantile_rounded) %>% 
  summarize(p_cum=max(P) %>% round(2)) %>% 
  mutate(n_total=nrow(reg$dfrXP),
         p_cum_prev = lag(p_cum) %>% replace_na(0),
         prop = p_cum-p_cum_prev,
         n = prop*n_total
         )
#df_sf_hist$counterfactual_freq_ft_1
est_probs
```

```{r}
est_dens = reg$dfrED %>% 
   mutate(n_total=nrow(reg$dfrED),
         #p_cum_prev = lag(p_cum) %>% replace_na(0),
         #prop = p_cum-p_cum_prev,
         #n = prop*n_total
         )
#df_sf_hist$counterfactual_freq_ft_1
est_dens
```






• regkienerLX, laplacegaussnorm.
• fitkienerX.
• paramkienerX, paramkienerX5, paramkienerX7

 model K1, K2, K3, K4

#### poweRlaw
#### fitdistrplus

##### Lognormal

```{r}
f1 <- fitdist(sf_unit_counts, "lnorm", method="mle") 
f2 <- fitdist(sf_unit_counts, "lnorm", method="mme")
par(mfrow=1:2)
cdfcomp(list(f1, f2), do.points=FALSE, xlogscale = TRUE, main = "CDF plot")
denscomp(list(f1, f2), demp=TRUE, main = "Density plot")
```
##### Pareto

Figure out reasonable starting values for Pareto model

```{r}
plotdist(distr = "pareto",para = list(shape=1,scale=5),
         data = sf_unit_counts)
```


```{r}
f3 <- fitdist(sf_unit_counts, "pareto",method="mle",
              start=list(shape=1,scale=5))
f4 <- fitdist(sf_unit_counts, "pareto", method="mme",
              start=list(shape=1,scale=5),order=1:2)
par(mfrow=1:2)
cdfcomp(list(f3, f4), do.points=FALSE, xlogscale = TRUE, main = "CDF plot")
denscomp(list(f3, f4), demp=TRUE, main = "Density plot")
```


#### sads


```{r}
df_sf_hist %>% 
  filter(number_of_units>=3,number_of_units<=100) %>% 
  ggplot() +
  geom_bar(aes(x=number_of_units,y=freq),stat='identity') +
  geom_line(aes(x=number_of_units,y=counterfactual_freq_ft_1),color='blue') +
  #geom_vline(xintercept=23.5,linetype='dashed',color='blue') +
  #geom_vline(xintercept=24.5,linetype='dashed',color='black') +
  scale_x_continuous(n.breaks=10) +
  labs(x = 'Number of Units per Building',
       y = 'Frequency',
       title = 'Frequency of Apartment Building Unit Counts in SF, 6/2016-4/2023')
```

### Pre-IZ Assumption Check

check to make sure no bunching before IZ start date

```{r}
df_sf %>% 
  filter(number_of_units>=3,
         number_of_units<=50,date_built<"2016-06-01") %>% 
  ggplot(aes(x=number_of_units)) + 
  geom_histogram(color='white',binwidth=1) +
  labs(x = 'Number of Units per Building',y='Number of Buildings',
       title = 'San Francisco Residential Building Permits, 10/1977-6/2016')
```

 
### Bootstrapped Hypothesis Testing

Residuals Plot
```{r}
df_sf_hist %>% 
  mutate(residual=freq-counterfactual_freq_pow,
         zero=0) %>% 
  ggplot(aes(x=number_of_units,xend=number_of_units,y=zero,yend=residual)) +
  geom_segment() + 
  geom_vline(xintercept=10,color='blue',linetype='dashed') +
  geom_vline(xintercept=25,color='blue',linetype='dashed') + 
  labs(x = 'Number of Units per Building',
       y = 'Residual',
       title = 'SF Model Residuals')
```
```{r}
n_bootstraps = 500
df_sf_boot = df_sf_hist %>% 
  mutate(residual=freq-counterfactual_freq_pow,
         threshold_num = 25,
          units_below_thresh=threshold_num-number_of_units,
         bunch_ind = as.numeric((units_below_thresh==1)|
           (units_below_thresh==2)&(threshold_num %% 2 == 0)),
         pre_bunch_ind = as.numeric(
           ((units_below_thresh %in% c(3,4))&(threshold_num %% 2 == 0))|
           ((units_below_thresh %in% c(2,3))&(threshold_num %% 2 == 1))
           )) 

n_pre_bunch_obs = df_sf_boot %>% filter(pre_bunch_ind==1) %>% pull(freq) %>% 
    sum
n_bunch_obs = df_sf_boot %>% filter(bunch_ind==1) %>% pull(freq) %>% 
    sum
n_diff_obs = n_bunch_obs-n_pre_bunch_obs

nrows = nrow(df_sf_boot)
boot_diffs = vector(length=n_bootstraps)
for (i in 1:n_bootstraps){
  df_sf_boot_i = df_sf_boot
  random_residuals_sample = sample(df_sf_boot_i$residual,
                                  size=nrows,
                                   replace = T)
  df_sf_boot_i$freq_boot = df_sf_boot_i$counterfactual_freq_pow+random_residuals_sample
  # calculate bunching statistic
  n_pre_bunch_i = df_sf_boot_i %>% filter(pre_bunch_ind==1) %>% pull(freq_boot) %>% 
    sum
  n_bunch_i = df_sf_boot_i %>% filter(bunch_ind==1) %>% pull(freq_boot) %>% 
    sum
  n_diff_i = n_bunch_i-n_pre_bunch_i
  boot_diffs[i] = n_diff_i
}
```

```{r}
n_diff_obs
p_val = length(boot_diffs[boot_diffs>n_diff_obs])/length(boot_diffs)
p_val
```

```{r}
hist(boot_diffs,main='Distribution of Bootstrapped Test Statistics',
     xlab = 'Test Statistic')
```

### McCrary Test

```{r}
DCdensity(runvar=df_sf %>% 
  filter(date_built<"2016-06-01") %>% 
    pull(number_of_units),cutpoint=25,ext.out=T)
```



## Portland, OR

```{r}
df_analysis %>% 
  filter(situs_city=='PORTLAND',situs_state=='OR',number_of_units>=3,
         number_of_units<=50) %>% 
  ggplot(aes(x=number_of_units)) + 
  geom_histogram(color='white',binwidth=1) +
  labs(title = 'Portland Residential Building Permits: 2017-2023',
       x = 'Number of Units per Building',y='Number of Buildings')
```
# Moderation Analysis

- understand heterogeneity in IZ effects
- % of units required to be affordable
- degree of affordability
- other 
