---
title: "IZ Bunching Analysis"
author: "Zachary Goldstein"
date: "2023-03-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
library(tidyverse)
library(readxl)
library(usdata)
library(magrittr)
options(dplyr.summarise.inform = FALSE)
```

# Loading Data

```{r}
df_cl = read_csv("data/corelogic_pb2_mf.csv")
dim(df_cl)
head(df_cl)
```
```{r}
df_iz_raw = read_excel("ih.org-inclusionary-database/Local Inclusionary Housing Policies.xlsx",
                 sheet="data",skip=1)
head(df_iz_raw)
```
# Cleaning/Processing

```{r}
df_pops = read_csv("https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv") %>% 
  mutate(state_code=state2abbr(State))
head(df_pops)
```

```{r}
df_iz = df_iz_raw %>% 
  select(`Program ID`,`Program Name`,`Government Name`,`Government Type`,
        state=`Government Address: State`,
        `Year Program Adopted`,`Year Program Updated`,
         `Program Applicable Areas`,`Minimum Project Size`,`Minimum Set-aside`,
         `Incentive`) %>% 
  rename_with(\(x){str_replace_all(x," ","_") %>% str_to_lower()}) %>% 
  # Only keep policies with known thresholds (3 units or less doesn't count)
  mutate(minimum_project_size = ifelse(minimum_project_size %in% c("Don't know","N/A","1 unit","2 units","3 units"),"missing",str_to_lower(minimum_project_size)),
         government_name = str_replace(government_name,
    "Town of |City of |City and County of |City Of |Town Of | Borough|Borough of ","") %>% 
      str_replace("Newark City","Newark"),
         state=str_replace(state,"D.C.","DC"),
    # Manual override policy details based on research
    # https://www.fbm.com/publications/what-you-need-to-know-about-san-franciscos-inclusionary-affordable-housing-program/
    minimum_project_size=ifelse(government_name=='San Francisco','25 units',minimum_project_size),
    year_program_adopted=ifelse(government_name=='San Francisco','2016',year_program_adopted)
    ) %>% 
  filter(minimum_project_size!="missing") %>% 
  left_join(df_pops %>% select(City,state_code,city_pop=Population),
            by=c("government_name"="City","state"="state_code"))

dim(df_iz)
head(df_iz)
```
```{r}
df_iz %<>% 
  mutate(minimum_project_size=str_replace(minimum_project_size,"1 million","1000000") %>% 
           str_replace(",",""),
  threshold_cat=case_when(
    str_detect(minimum_project_size,"^\\d+ units$") ~ "units",
    str_detect(minimum_project_size,"^\\d+ acres \\(parcel\\)$") ~ "acres_parcel",
    str_detect(minimum_project_size,"^\\d+ sqft \\(parcel\\)$") ~ "sqft_parcel",
    str_detect(minimum_project_size,"^\\d+ sqft \\(building\\)$") ~ "sqft_building",
    T ~ "other")) %>% 
  filter(threshold_cat!='other') %>% 
  mutate(threshold_num = str_extract(minimum_project_size, "[0-9]+\\.*[0-9]*") %>% 
           as.numeric)
df_iz %>% 
  distinct(minimum_project_size,threshold_cat,threshold_num) %>% 
  arrange(threshold_cat,minimum_project_size)
```
```{r}
# Remove duplicate municipalities in IZ dataset
df_iz %>% 
  count(government_name,state) %>%
  arrange(desc(n)) %>% 
  head()

# Need to come up with better way to pick among duplicates
df_iz %<>% 
  arrange(desc(threshold_num)) %>% 
  distinct(government_name,state,.keep_all = T)
```

```{r}
dim(df_iz)
head(df_iz)
```

## Single City Datasets
Combine CoreLogic with other property datasets


### Portland, OR
```{r}
# Portland OR isn't in the non-historical CL at all so can easily combine without de-duping
df_pd = read_csv("data/PDX_Residential_Building_Permits_20230328.csv",
      col_select =c("STATUS","YEAR_","NEWCLASS","NEWTYPE","NEW_UNITS",
                           "IS_ADU")) %>% 
      filter(NEW_UNITS>=2,IS_ADU==F) %>% 
      mutate(municipality_name="PORTLAND",
             situs_city="PORTLAND",
             situs_county="MULTNOMAH",
             situs_state="OR",
             number_of_buildings=1,
             number_of_units=NEW_UNITS,
             fips_code="41051",
             year_built=YEAR_) %>% 
      select(any_of(names(df_cl)))
```
### San Diego
```{r}
max_year_cl_sd = df_cl %>% 
                filter(situs_city=='SAN DIEGO',situs_state=='CA') %>% 
                pull(year_built) %>% 
                max(na.rm=T)
df_sd_raw = read_csv("data/san_diego_parcels.csv",
                     col_select = c("UNITQTY","YEAR_EFFECTIVE"),
                     show_col_types=F)
df_sd = df_sd_raw %>% 
   mutate(municipality_name="SAN DIEGO",
             situs_city="SAN DIEGO",
             situs_county="SAN DIEGO",
             situs_state="CA",
            # situs_zip_code=SITUS_ZIP,
             number_of_units=UNITQTY,
             number_of_buildings=1,
             fips_code="06073",
             year_built=ifelse(as.numeric(YEAR_EFFECTIVE)<=23,
                               2000+as.numeric(YEAR_EFFECTIVE),
                               1900+as.numeric(YEAR_EFFECTIVE))) %>% 
      select(any_of(names(df_cl))) %>% 
      filter(number_of_units>=2,year_built>max_year_cl_sd)
head(df_sd)
```
### San Francisco
```{r}
df_sf_raw = read_csv("data/sf_building_permits_20230404.csv",
                     show_col_types=F,
    col_select = c('Permit Type Definition','Description','Existing Use','Proposed Use',
                              'Permit Creation Date',
                             'Current Status',
                              'Current Status Date',
  'Filed Date','Issued Date','Completed Date','First Construction Document Date',
                             'Existing Units','Proposed Units'))
df_sf = df_sf_raw %>% 
   mutate(municipality_name="SAN FRANCISCO",
             situs_city="SAN FRANCISCO",
             situs_county="SAN FRANCISCO",
             situs_state="CA",
             number_of_units=`Proposed Units`,
             number_of_buildings=1,
             fips_code="06075",
          # Using filed date instead of year built because it makes more sense 
          # based on expected IZ developer behavior
            date_built = as.Date(`Filed Date`,format="%m/%d/%Y"),
             year_built=as.numeric(format(as.Date(`Filed Date`, format = "%m/%d/%Y"), format = "%Y"))) %>% 
      filter(number_of_units>=2,str_detect(`Permit Type Definition`,"new construction")) %>% 
      select(any_of(names(df_cl)),date_built)
head(df_sf)
```
### San Jose
```{r}
# Just active bulding permits, so shouldn't overlap with CL
df_sj_raw = read_csv("data/san_jose_buildingpermitsactive.csv",
                     show_col_types=F)
df_sj = df_sj_raw %>% 
   mutate(municipality_name="SAN JOSE",
             situs_city="SAN JOSE",
             situs_county="SANTA CLARA",
             situs_state="CA",
             number_of_units=DWELLINGUNITS,
             number_of_buildings=1,
             fips_code="06085",
             date_built = as.Date(ISSUEDATE,format="%m/%d/%Y"),
             year_built=as.numeric(format(as.Date(ISSUEDATE, format = "%m/%d/%Y"), format = "%Y"))) %>% 
      filter(number_of_units>=2,WORKDESCRIPTION=='New Construction') %>% 
      select(any_of(names(df_cl)),date_built)
head(df_sj)
```

### Denver

```{r}
df_dn_raw = read_csv("data/denver_real_property_residential_characteristics.csv",
                     show_col_types=F,col_select=c("UNITS","CCYRBLT"))
df_dn = df_dn_raw %>% 
   mutate(municipality_name="DENVER",
             situs_city="DENVER",
             situs_county="DENVER",
             situs_state="CO",
             number_of_units=UNITS,
             number_of_buildings=1,
             fips_code="08031",
             year_built=CCYRBLT) %>% 
      filter(number_of_units>=2) %>% 
      select(any_of(names(df_cl)))
head(df_dn)
```

### Minneapolis

```{r}
max_year_cl_mn = df_cl %>% 
                filter(situs_city=='MINNEAPOLIS',situs_state=='MN') %>% 
                pull(year_built) %>% 
                max(na.rm=T)
df_mn_raw = read_csv("data/minneapolis_permits_20230412.csv",
                     show_col_types=F,col_select=c("dwellingUnitsNew","issueDate"))
df_mn = df_mn_raw %>% 
   mutate(municipality_name="MINNEAPOLIS",
             situs_city="MINNEAPOLIS",
             situs_county="HENNEPIN",
             situs_state="MN",
             number_of_units=dwellingUnitsNew,
             number_of_buildings=1,
             fips_code="27053",
             date_built = as_date(ymd_hms(issueDate)),
             year_built= year(ymd_hms(issueDate))) %>% 
      filter(number_of_units>=2,year_built>max_year_cl_mn) %>% 
      select(any_of(names(df_cl)),date_built)
head(df_mn)
```

### New Jersey
```{r}
# There's no NJ at all in CL, so no duplicates
df_nj_raw = read_csv("data/NJ_permits_20230412.csv",
                     show_col_types=F,col_select=c("Sale Units Gained",
          "Rental Units Gained","Municipality Name","County","Permit Date"))
df_nj = df_nj_raw %>% 
   mutate(municipality_name=`Municipality Name`,
             situs_city=`Municipality Name`,
             situs_county=`County`,
             situs_state="NJ",
             number_of_units=`Sale Units Gained`+`Rental Units Gained`,
             number_of_buildings=1,
            # fips_code=,
             date_built = mdy(`Permit Date`),
             year_built= year(mdy(`Permit Date`))) %>% 
      filter(number_of_units>=2) %>% 
      select(any_of(names(df_cl)),date_built)
head(df_nj)
```



## Data Quality Check

Compare CoreLogic to other data sources
```{r}
# SF
df_sf %>% 
  head()
df_cl_sf_recent = df_cl %>% 
  filter(situs_city=='SAN FRANCISCO',
         year_built>= (df_sf %>% pull(year_built) %>% min))
df_cl_sf_recent %>% 
  head()
```
```{r}
df_cl_sf_recent %>% nrow
```

```{r}
df_sf %>% nrow
```

```{r}
df_cl_sf_recent %>% 
  filter(number_of_units<=50,number_of_units>=4) %>% 
  ggplot(aes(x=number_of_units)) +
  geom_histogram(bins=40,color='white')
```

```{r}
# Exclude cities where I prefer to just use non-corelogic dataset to avoid dupes
cities_to_exclude_from_cl = "SAN FRANCISCO"
df_properties = df_cl %>% 
  filter(situs_city %in% cities_to_exclude_from_cl == F) %>% 
  bind_rows(df_pd) %>% 
  bind_rows(df_sd) %>% 
  bind_rows(df_sf) %>% 
  bind_rows(df_sj) %>% 
  bind_rows(df_dn) %>% 
  bind_rows(df_mn) %>% 
  bind_rows(df_nj)
df_properties %>% 
  tail()
```


```{r}
# Check for duplicates and missing values (linkage failures)
df_merged = df_properties %>% 
  mutate(situs_city=coalesce(situs_city,municipality_name),
         situs_county=str_replace(situs_county," COUNTY","")) %>% 
  full_join(df_iz %>% 
              filter(government_type %in% c("Town or city","Special district")) %>% 
              mutate(government_name=str_to_upper(government_name)),
        by=c("situs_city"="government_name","situs_state"="state"),keep=T) %>% 
  full_join(df_iz %>% 
              filter(government_type %in% c("Consolidated city-county","County")) %>% 
              mutate(government_name=str_to_upper(government_name) %>% 
                       str_replace("COUNTY OF ","") %>% 
                       str_replace(" COUNTY","")),
            by=c("situs_county"="government_name","situs_state"="state"),keep=T,suffix=c("_xcity","_ycounty"))
# Assert there are no properties that match to more than 1 IZ policy
# Need to address edge cases here, like San Mateo County
# stopifnot(df_merged %>% filter(is.na(government_name_xcity)==F&is.na(government_name_ycounty)==F) %>% nrow == 0)
# df_merged %>% filter(is.na(government_name_xcity)==F&is.na(government_name_ycounty)==F) %>% 
#   distinct(government_name_xcity,government_name_ycounty)

# Coalesce cols together
cols_to_coalesce = names(df_merged)
for (i in seq_along(cols_to_coalesce)){
  x_col_name = cols_to_coalesce[i]
  if (str_ends(x_col_name,"_xcity")){
    new_col_name = str_replace(x_col_name,"_xcity","")
    y_col_name = str_replace(x_col_name,"_xcity","_ycounty")
    df_merged = df_merged %>% 
      mutate(!!new_col_name := coalesce(!!sym(x_col_name), !!sym(y_col_name)))
  }}

df_merged = df_merged %>% select(!contains("_xcity")&!contains("_ycounty"))
df_merged %>% 
  select(situs_city,situs_county,situs_state,government_name,government_type,state) %>% 
  filter(is.na(government_name)==F) %>% 
  distinct(situs_city,situs_county,situs_state,government_name,government_type,state) %>% 
  slice_sample(n=20)
```

## Missing data check
Check for IZ policies with missing property data

Note: I looked for the top 10 missing IZ cities by city pop and didn't find in non-historical CL data
```{r}
df_missing = df_merged %>% 
  filter(is.na(clip)) %>% 
  distinct(government_name,government_type,state,city_pop) %>% 
  arrange(desc(city_pop))
df_missing %>% 
  head(10)
```

## Edge cases to handle
Handle San Mateo, Santa Clara edge cases with double requirement at city + county level
```{r}
df_merged %>% filter(str_detect(situs_county,"SAN MAT")) %>% 
  select(situs_city,situs_county,situs_state,government_name) %>% 
  distinct %>% 
  head()
```

## Prep data for analysis
```{r}
df_analysis = df_merged %>% 
  mutate(bldg_year_combined=coalesce(effective_year_built,year_built),
         bldg_sq_ft_combined = coalesce(universal_building_square_feet,building_square_feet)) %>% 
  filter(is.na(bldg_year_combined)==F,is.na(threshold_cat)==F,number_of_buildings==1,
         # Either date of adoption unknown, or the building was built after IZ adoption
         (bldg_year_combined>=year_program_adopted|year_program_adopted=="Don't know")) %>% 
  select(situs_city,situs_county,situs_state,government_name,government_type,
        acres,land_square_footage,bldg_sq_ft_combined,
        bldg_year_combined,number_of_units,number_of_buildings,
        threshold_cat,threshold_num,year_program_adopted,year_program_updated)
df_analysis %>% 
  slice_sample(n=10)
df_analysis %>% dim
```

```{r}
df_analysis %>% 
  count(situs_city,situs_county,situs_state,threshold_cat,threshold_num) %>% 
  arrange(desc(n)) %>% 
  head(10)
```

```{r}
df_units = df_analysis %>% 
  filter(threshold_cat=="units",threshold_num>=5) %>% 
  mutate(rel_units=number_of_units-threshold_num,
         units_below_thresh=threshold_num-number_of_units)
dim(df_units)
df_units %>% head()
```
# EDA

## Counts

```{r}
# How many IZ programs are in the inclusionary housing spreadsheet?
df_iz_raw %>% nrow
```

```{r}
# How many IZ programs in the spreadsheet have a threshold?
df_iz %>% 
  count(threshold_cat) %>% 
  arrange(desc(n))
```

```{r}
# How many unique jurisdictions have IZ programs with a threshold?
df_iz %>% 
  pull(government_name) %>% 
  unique() %>% 
  length
```

```{r}
# How many IZ jurisdictions overlap with Property Data (CL + other)?
df_merged %>% 
  filter(is.na(number_of_units)==F) %>% 
  pull(government_name) %>% 
  unique() %>% 
  length
```

```{r}
# How many IZ jurisdictions overlap with Property Data (CL + other) if we limit to 
# properties from after the start of the IZ program?
df_analysis %>% 
  pull(government_name) %>% 
  unique() %>% 
  length
```

```{r}
# How many properties are in that dataset?
df_analysis %>% nrow
```

```{r}
# And which cities are they in?
df_analysis %>% 
  count(situs_city,situs_state) %>% 
  arrange(desc(n)) %>% 
  head(10)
```


# National Analysis
```{r}
ggplot(df_units %>% filter(number_of_units<=50,threshold_num<=40,threshold_num>=10),
       aes(x=rel_units)) +
  geom_histogram(bins=150,color='white')
```

```{r}
df_units %>% 
  mutate(bunch_ind = as.numeric((units_below_thresh==1)|
           (units_below_thresh==2)&(threshold_num %% 2 == 0)),
         pre_bunch_ind = as.numeric(
           ((units_below_thresh %in% c(3,4))&(threshold_num %% 2 == 0))|
           ((units_below_thresh %in% c(2,3))&(threshold_num %% 2 == 1))
           ),
         post_bunch_ind = as.numeric(units_below_thresh==0),
         one_below_ind = as.numeric(units_below_thresh==1),
         ) %>% 
  filter(threshold_num>=7) %>% 
  group_by(situs_city,situs_state,threshold_num) %>% 
  summarize(n_bunch=sum(bunch_ind),
            n_pre_bunch = sum(pre_bunch_ind),
            n_post_bunch = sum(post_bunch_ind),
            n_one_below = sum(one_below_ind),
            n_total=n(),
            n_diff=n_bunch-n_pre_bunch) %>% 
  arrange(desc(n_diff)) %>% 
  # If IZ takes effect at unit counts >=X...
  # ... there should be more buildings of size X-1 than of X in the jurisdiction
  filter(n_post_bunch<n_one_below) %>% 
  head(10)
```
```{r}
single_eda_plot = function(city,state,data=df_units){
  data_filtered = df_units %>% filter(situs_city==city,situs_state==state,
                      rel_units<=50)
  n_bins = max(data_filtered$number_of_units)-min(data_filtered$number_of_units)
  thresh = data_filtered$threshold_num %>% first
ggplot(data_filtered,
       aes(x=number_of_units)) +
  geom_histogram(bins=n_bins,color='white') +
  geom_vline(xintercept=thresh,color='red') + 
  labs(title=str_glue("Distribution of Apartment Building Unit Counts in {city}, {state}"),
       y="Frequency",x = "Units per Apartment Building")
}
single_eda_plot(city="DENVER",state="CO")
```


```{r}
sq_ft_per_acre = 43560
df_parcel_area = df_analysis %>% 
  filter(threshold_cat %in% c("acres_parcel","sqft_parcel")) %>% 
  mutate(threshold_acres = ifelse(threshold_cat=='acres_parcel',threshold_num,
                                  threshold_num/sq_ft_per_acre),
         rel_acres = acres-threshold_acres)
dim(df_parcel_area)
df_parcel_area %>% 
  head()
```

```{r}
df_parcel_area %>% 
  filter(acres<50) %>% 
  ggplot(aes(x=rel_acres)) +
  geom_histogram(color='white')
```

```{r}
df_building_area = df_analysis %>% 
  filter(threshold_cat=='sqft_building')
df_building_area %>% 
  head()
```

# Case Study ID
Promising candidates for single-city datasets

Cities missing from CL completely
```{r}
df_merged %>% 
  filter(is.na(clip),threshold_cat=="units",threshold_num>=6,city_pop>=100000,
         year_program_adopted<=2020) %>% 
  distinct(government_name,state,city_pop,threshold_num,threshold_cat) %>% 
  arrange(desc(city_pop))
```
Cities missing recent data from CL (post-IZ adoption)
```{r}
cl_has_post_adoption = df_merged %>% 
  filter(year_built>year_program_adopted,city_pop>100000,threshold_num>=6,
         threshold_cat=="units",year_program_adopted<=2020|year_program_adopted=="Don't know") %>% 
  distinct(situs_city,situs_state,city_pop) %>% 
  arrange(desc(city_pop)) %>% 
  head(20)
```

```{r}
cl_has_any_time = df_merged %>% 
  filter(is.na(clip)==F,city_pop>100000,threshold_num>=6,
         threshold_cat=="units",year_program_adopted<=2020|year_program_adopted=="Don't know") %>% 
  distinct(situs_city,situs_state,city_pop) %>% 
  arrange(desc(city_pop)) %>% 
  head(20)
```

```{r}
cl_has_any_time %>% 
  left_join(cl_has_post_adoption,by=c("situs_city","situs_state")) %>% 
  filter(is.na(city_pop.y)==T)
```

```{r}
df_merged %>% 
  filter(government_type!='Town or city',threshold_cat=='units',threshold_num>=6,
         is.na(number_of_units),year_program_adopted<=2020|year_program_adopted=="Don't know") %>% 
  pull(program_id)
```

```{r}
df_iz %>% 
  filter(threshold_num>=10,threshold_cat=='units') %>% 
  arrange(desc(city_pop)) %>% 
  head()
```

```{r}
df_iz %>% filter(government_name=='San Francisco')
```

# Case Study Analysis
## San Francisco, CA

```{r}
df_sf %>% 
  filter(number_of_units>=2,
         number_of_units<=50,date_built>="2016-06-01") %>% 
  ggplot(aes(x=number_of_units)) + 
  geom_histogram(color='white',bins=45)
```
```{r}
min_units = 3
max_units = 100
sf_post_start_date = "2016-06-08"
df_sf_hist = df_sf %>% 
  filter(number_of_units>=min_units,
         number_of_units<=max_units,date_built>=sf_post_start_date) %>% 
  count(number_of_units,name='freq') %>% 
  # Add missing zero frequencies
  complete(number_of_units=min_units:max_units,fill=list(freq=0)) %>% 
  mutate(number_of_units_inverse=1/number_of_units,
         number_of_units_inverse_squared = 1/number_of_units^2)
sf_counterfactual_max = 23
df_sf_hist_train = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max)

sf_counterfactual_model_inv = lm(freq~number_of_units_inverse+number_of_units_inverse_squared,
                             data=df_sf_hist_train)
sf_counterfactual_model_exp = nls(freq~a*exp(-b*number_of_units),
                            data=df_sf_hist_train, start = list(a = 1000, b = 0.5))
sf_counterfactual_model_pow = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train, start = list(a = 1000, b = 0.5))

df_sf_hist$counterfactual_freq_inv = predict(sf_counterfactual_model_inv,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_exp = predict(sf_counterfactual_model_exp,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow = predict(sf_counterfactual_model_pow,df_sf_hist) %>% round(1)

df_sf_hist
```
```{r}
df_sf_hist %>% 
  filter(number_of_units>=3,number_of_units<=100) %>% 
  ggplot() +
  geom_bar(aes(x=number_of_units,y=freq),stat='identity') +
  geom_line(aes(x=number_of_units,y=counterfactual_freq_pow),color='blue') +
  geom_vline(xintercept=23.5,linetype='dashed',color='blue') +
  geom_vline(xintercept=24.5,linetype='dashed',color='black') +
  scale_x_continuous(n.breaks=10) +
  labs(x = 'Number of Units per Building',
       y = 'Frequency',
       title = 'Frequency of Apartment Building Unit Counts in SF, 6/2016-4/2023')
```


### Interpolation Approach

```{r}
df_sf_hist_train_interpol_2 = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max|number_of_units>=sf_counterfactual_max*1.5)
df_sf_hist_train_interpol_3 = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max|number_of_units>=sf_counterfactual_max*2)
df_sf_hist_train_interpol_4 = df_sf_hist %>% 
  filter(number_of_units<=sf_counterfactual_max|number_of_units>=sf_counterfactual_max*3)

sf_counterfactual_model_pow_interpol_2 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_2, start = list(a = 1000, b = 0.5))
sf_counterfactual_model_pow_interpol_3 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_3, start = list(a = 1000, b = 0.5))
sf_counterfactual_model_pow_interpol_4 = nls(freq~a*number_of_units^(-b),
                              data=df_sf_hist_train_interpol_4, start = list(a = 1000, b = 0.5))

df_sf_hist$counterfactual_freq_pow_interpol_2 = predict(sf_counterfactual_model_pow_interpol_2,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow_interpol_3 = predict(sf_counterfactual_model_pow_interpol_3,df_sf_hist) %>% round(1)
df_sf_hist$counterfactual_freq_pow_interpol_4 = predict(sf_counterfactual_model_pow_interpol_4,df_sf_hist) %>% round(1)

df_sf_hist
```

```{r}
df_sf_hist %>% 
  filter(number_of_units>=3,number_of_units<=100) %>% 
  ggplot() +
  geom_bar(aes(x=number_of_units,y=freq),stat='identity') +
  geom_line(aes(x=number_of_units,y=counterfactual_freq_pow_interpol_2),color='blue') +
  geom_vline(xintercept=23.5,linetype='dashed',color='blue') +
  geom_vline(xintercept=24.5,linetype='dashed',color='black') +
  scale_x_continuous(n.breaks=10) +
  labs(x = 'Number of Units per Building',
       y = 'Frequency',
       title = 'Frequency of Apartment Building Unit Counts in SF, 6/2016-4/2023')
```

Interpolation only slightly increases fatness of tails. Need to try different 
distribution.
```{r}
df_sf_hist %>% 
  summarize(n_actual=sum(number_of_units*freq),
            n_counterfactual=sum(number_of_units*counterfactual_freq_pow),
            sum(number_of_units*counterfactual_freq_pow_interpol_2),
            sum(number_of_units*counterfactual_freq_pow_interpol_3),
            sum(number_of_units*counterfactual_freq_pow_interpol_4))
```


## Portland, OR

```{r}
df_analysis %>% 
  filter(situs_city=='PORTLAND',situs_state=='OR',number_of_units>=3,
         number_of_units<=50) %>% 
  ggplot(aes(x=number_of_units)) + 
  geom_histogram(color='white',bins=45)
```

